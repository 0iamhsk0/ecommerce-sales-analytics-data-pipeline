# ğŸ“¦ E-Commerce Sales Analytics Data Pipeline
**Databricks | Delta Lake | Medallion Architecture**

---

## ğŸ“Œ Project Overview
This project implements an **end-to-end E-Commerce data analytics pipeline** using **Databricks** and **Delta Lake**, following the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.

In addition to the standard pipeline, the project includes:
- Data quality validation and governance
- Quarantine handling for invalid records
- Incremental processing logic (job-ready design)
- Analytics-ready Gold views
- Business-focused dashboards for validation and insights

> ğŸ¯ **Goal:** Demonstrate real-world **Data Engineering best practices**, not just analytics.

---

## ğŸ§± Architecture Overview

```text
Source Data
   â†“
Bronze Layer (Raw Ingestion)
   â†“
Silver Layer (Cleaned + Validated)
   â†“
Gold Layer (Analytics & Business Views)
   â†“
Dashboards (Insights & Validation)
```

---

## ğŸ¥‰ Bronze Layer â€“ Raw Ingestion
- Stores raw e-commerce data in **Delta format**
- No transformations applied
- Preserves original schema and values
- Acts as a recovery and audit layer

---

## ğŸ¥ˆ Silver Layer â€“ Cleansing & Data Quality
The Silver layer prepares data for analytics and governance.

### âœ” Key Features
- Schema standardization  
- Null and value validation  
- Referential integrity checks  
- Duplicate detection  

### ğŸ” Data Quality Checks
Implemented using **Spark SQL notebooks**:
- `invalid_customers`
- `duplicate_customers`
- `invalid_products`
- `invalid_order_items`

ğŸ“ **Location:**  
`/data_quality`

### ğŸš§ Quarantine Handling
Invalid records are isolated into **quarantine tables** with rejection reasons:
- Prevents bad data from entering Gold
- Enables traceability and auditability
- Mirrors real-world data governance workflows

---

## ğŸ¥‡ Gold Layer â€“ Analytics & Business Modeling

### ğŸ“Š Core Gold Tables
- `gld_fact_order_items`
- `gld_dim_customers`
- `gld_dim_products`
- `gld_dim_date`
- `fact_transactions_denorm` *(denormalized analytics table)*

ğŸ“ **Location:**
- Dimensions â†’ `/medallion_processing_dim`
- Facts â†’ `/medallion_processing_fact`

### ğŸ“ˆ Analytics Views (Gold)
Instead of duplicating tables, **analytics views** are created on top of Gold facts:
- Daily sales performance
- Customer lifetime value
- Channel-wise performance
- Coupon / discount impact analysis

âœ” Improves maintainability  
âœ” Reduces storage overhead  
âœ” Follows analytics engineering best practices  

---

## ğŸ” Incremental Processing Design
Although the dataset is static, the pipeline is designed to be **incremental-ready**:
- Metadata-based watermark logic
- Idempotent transformations
- Re-runnable notebooks without duplication

ğŸ“Œ **Design decision:**  
Jobs and scheduled pipelines were intentionally not created, as the dataset does not change.  
The pipeline is **job-ready** and can be scheduled if real-time or periodic data sources are introduced.

---

## ğŸ“Š Dashboards

### 1ï¸âƒ£ Core Medallion Analytics Dashboard
**Purpose:**  
Validate the original Medallion pipeline and Gold modeling.

**Key Insights:**
- Monthly sales trend
- Revenue by category
- Time-based sales patterns

ğŸ“ **Assets:**  
`/dashboards`

---

### 2ï¸âƒ£ Data Quality & Analytics Validation Dashboard
**Purpose:**  
Demonstrate how data quality and governance improve business analytics.

**Includes:**
- KPI cards (Total Revenue, Total Transactions, Avg Transaction Value)
- Revenue trends
- Channel-wise revenue
- Top customers by lifetime value
- Coupon vs non-coupon revenue comparison

This dashboard directly links **data engineering decisions â†’ business impact**.

---

## ğŸ—‚ï¸ Project Structure

```text
ecommerce-sales-analytics-data-pipeline/
â”œâ”€â”€ setup/                     # Initial setup & configurations
â”œâ”€â”€ medallion_processing_dim/  # Dimension table processing
â”œâ”€â”€ medallion_processing_fact/ # Fact table processing
â”œâ”€â”€ data_quality/              # Validation & quarantine logic
â”œâ”€â”€ dashboards/                # Dashboard-related assets
â”œâ”€â”€ README.md
```
---

## ğŸ› ï¸ Technologies Used
- Databricks  
- Apache Spark  
- Delta Lake  
- Databricks SQL Dashboards  
- GitHub (Databricks Repos integration)

---

## ğŸ¯ Key Learnings & Outcomes
- Implemented a complete **Medallion Architecture**
- Applied **data quality & governance** patterns
- Designed **analytics-ready Gold views**
- Built **business-facing dashboards**
- Practiced **incremental & job-ready pipeline design**

---

## ğŸ“Œ Disclaimer
This project uses a **static dataset** for learning and portfolio purposes.  
Pipeline orchestration and scheduling were intentionally omitted to reflect **realistic engineering decisions**.



